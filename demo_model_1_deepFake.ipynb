{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Maww6Afnn5_i"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQoPtrprn87O"},"outputs":[],"source":["!pip3 install face_recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiM1o1XFn-7G"},"outputs":[],"source":["import glob\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","# import face_recognition\n","#Check if the file is corrupted or not\n","def validate_video(vid_path,train_transforms):\n","      transform = train_transforms\n","      count = 20\n","      video_path = vid_path\n","      frames = []\n","      a = int(100/count)\n","      first_frame = np.random.randint(0,a)\n","      temp_video = video_path.split('/')[-1]\n","      for i,frame in enumerate(frame_extract(video_path)):\n","        frames.append(transform(frame))\n","        if(len(frames) == count):\n","          break\n","      frames = torch.stack(frames)\n","      frames = frames[:count]\n","      return frames\n","#extract a from from video\n","def frame_extract(path):\n","  vidObj = cv2.VideoCapture(path)\n","  success = 1\n","  while success:\n","      success, image = vidObj.read()\n","      if success:\n","          yield image\n","\n","im_size = 112\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transforms = transforms.Compose([\n","                                        transforms.ToPILImage(),\n","                                        transforms.Resize((im_size,im_size)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean,std)])\n","video_fil = glob.glob('/content/drive/MyDrive/deep fake/FF_Face_only_data/*.mp4')\n","print(\"Total no of videos :\" , len(video_fil))\n","print(video_fil)\n","count = 0;\n","for i in video_fil:\n","  try:\n","    count+=1\n","    validate_video(i,train_transforms)\n","  except:\n","    print(\"Number of video processed: \" , count ,\" Remaining : \" , (len(video_fil) - count))\n","    print(\"Corrupted video is : \" , i)\n","    continue\n","print((len(video_fil) - count))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2faPkfJroDm4"},"outputs":[],"source":["import json\n","import glob\n","import numpy as np\n","import cv2\n","import copy\n","import random\n","# video_files =  glob.glob('/content/drive/My Drive/Celeb_fake_face_only/*.mp4')\n","# video_files += glob.glob('/content/drive/My Drive/Celeb_real_face_only/*.mp4')\n","# video_files += glob.glob('/content/drive/My Drive/DFDC_FAKE_Face_only_data/*.mp4')\n","# video_files += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n","video_files = glob.glob('/content/drive/MyDrive/deep fake/FF_Face_only_data/*.mp4')\n","random.shuffle(video_files)\n","random.shuffle(video_files)\n","frame_count = []\n","for video_file in video_files:\n","  cap = cv2.VideoCapture(video_file)\n","  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\u003c100):\n","    video_files.remove(video_file)\n","    continue\n","  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n","print(\"frames are \" , frame_count)\n","print(\"Total no of video: \" , len(frame_count))\n","print('Average frame per video:',np.mean(frame_count))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZImCJwkoMyc"},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","# import face_recognition\n","class video_dataset(Dataset):\n","    def __init__(self,video_names,labels,sequence_length = 60,transform = None):\n","        self.video_names = video_names\n","        self.labels = labels\n","        self.transform = transform\n","        self.count = sequence_length\n","    def __len__(self):\n","        return len(self.video_names)\n","    def __getitem__(self,idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        a = int(100/self.count)\n","        first_frame = np.random.randint(0,a)\n","        temp_video = video_path.split('/')[-1]\n","        #print(temp_video)\n","        label = self.labels.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n","        if(label == 'FAKE'):\n","          label = 0\n","        if(label == 'REAL'):\n","          label = 1\n","        for i,frame in enumerate(self.frame_extract(video_path)):\n","          frames.append(self.transform(frame))\n","          if(len(frames) == self.count):\n","            break\n","        frames = torch.stack(frames)\n","        frames = frames[:self.count]\n","        #print(\"length:\" , len(frames), \"label\",label)\n","        return frames,label\n","    def frame_extract(self,path):\n","      vidObj = cv2.VideoCapture(path)\n","      success = 1\n","      while success:\n","          success, image = vidObj.read()\n","          if success:\n","              yield image\n","#plot the image\n","def im_plot(tensor):\n","    image = tensor.cpu().numpy().transpose(1,2,0)\n","    b,g,r = cv2.split(image)\n","    image = cv2.merge((r,g,b))\n","    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n","    image = image*255.0\n","    plt.imshow(image.astype(int))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrQ-JL5JoQBb"},"outputs":[],"source":["#count the number of fake and real videos\n","def number_of_real_and_fake_videos(data_list):\n","  header_list = [\"file\",\"label\"]\n","  lab = pd.read_csv('/content/drive/My Drive/deep fake/FF_Face_only_data/metadata.csv',names=header_list)\n","  fake = 0\n","  real = 0\n","  for i in data_list:\n","    temp_video = i.split('/')[-1]\n","    label = lab.iloc[(labels.loc[labels[\"file\"] == temp_video].index.values[0]),1]\n","    if(label == 'FAKE'):\n","      fake+=1\n","    if(label == 'REAL'):\n","      real+=1\n","  return real,fake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCX5ARCdoQlU"},"outputs":[],"source":["# load the labels and video in data loader\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","header_list = [\"file\",\"label\"]\n","labels = pd.read_csv('/content/drive/My Drive/deep fake/FF_Face_only_data/metadata.csv',names=header_list)\n","#print(labels)\n","train_videos = video_files[:int(0.7*len(video_files))]\n","valid_videos = video_files[int(0.7*len(video_files)):]\n","print(\"train : \" , len(train_videos))\n","print(\"test : \" , len(valid_videos))\n","# train_videos,valid_videos = train_test_split(data,test_size = 0.3)\n","# print(train_videos)\n","\n","print(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_videos)[0],\" Fake:\",number_of_real_and_fake_videos(train_videos)[1])\n","print(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(valid_videos)[0],\" Fake:\",number_of_real_and_fake_videos(valid_videos)[1])\n","\n","\n","im_size = 112\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","train_transforms = transforms.Compose([\n","                                        transforms.ToPILImage(),\n","                                        transforms.Resize((im_size,im_size)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean,std)])\n","\n","test_transforms = transforms.Compose([\n","                                        transforms.ToPILImage(),\n","                                        transforms.Resize((im_size,im_size)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean,std)])\n","train_data = video_dataset(train_videos,labels,sequence_length = 10,transform = train_transforms)\n","#print(train_data)\n","val_data = video_dataset(valid_videos,labels,sequence_length = 10,transform = train_transforms)\n","train_loader = DataLoader(train_data,batch_size = 4,shuffle = True,num_workers = 4)\n","valid_loader = DataLoader(val_data,batch_size = 4,shuffle = True,num_workers = 4)\n","image,label = train_data[0]\n","im_plot(image[0,:,:,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYTgCJj_obyD"},"outputs":[],"source":["# Get a sample batch from the training DataLoader\n","inputs, targets = next(iter(train_loader))\n","\n","# Print dimensions, shapes, and sizes\n","print(\"Training Data:\")\n","print(\"Inputs - Dimensions:\", inputs.dim(), \"Shape:\", inputs.shape, \"Size:\", inputs.size())\n","print(\"Targets - Dimensions:\", targets.dim(), \"Shape:\", targets.shape, \"Size:\", targets.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"it22WrLwpm4d"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","# Step 1: Initialize the pre-trained ResNeXt-50 model\n","class ResNeXtLSTM(nn.Module):\n","    def __init__(self, num_classes, hidden_dim=2048, lstm_layers=1):\n","        super(ResNeXtLSTM, self).__init__()\n","        resnext = models.resnext50_32x4d(pretrained=True)\n","        self.features = nn.Sequential(*list(resnext.children())[:-2])  # Remove last two layers (avgpool and fc)\n","        self.lstm = nn.LSTM(2048, hidden_dim, lstm_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, c, h, w = x.shape\n","        x = x.view(batch_size * seq_length, c, h, w)\n","        x = self.features(x)\n","        x = x.view(batch_size, seq_length, -1)\n","        _, (h_n, _) = self.lstm(x)\n","        output = self.linear(h_n[-1])  # Use only the last output of the LSTM\n","        return output\n","\n","# Step 2: Instantiate the model\n","model = ResNeXtLSTM(num_classes=2)\n","\n","# Step 3: Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Step 4: Train the model\n","def train_epoch(model, train_loader, criterion, optimizer):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_acc = correct / total\n","    return epoch_loss, epoch_acc\n","\n","# Step 5: Test the model\n","def test_epoch(model, test_loader, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / len(test_loader)\n","    epoch_acc = correct / total\n","    return epoch_loss, epoch_acc\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPzzS7Mt0JmOkpYe+JHrzJw","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}